# COLLECTION OF AIMS SPECIFIC SCRIPTS AND METHODS
# This assumes that a connection to a database with properly configured
# tables for thucs is available

from pathlib import Path

import itertools

import copy
from multiprocessing.pool import ThreadPool
from functools import partial

import json

import pandas as pd
import geopandas as gpd

from sqlalchemy import create_engine, text as sql_text
from sqlalchemy import URL

from pyagnps import annagnps, utils, constants, climate

from tqdm import tqdm

class AIMSWatershed:
    def __init__(self, path_to_json_creds, **kwargs):
        credentials = open_creds_dict(path_to_json_creds)
        url_object = create_db_url_object(credentials)

        self.engine = create_engine(url_object)
        self.path_to_json_creds = Path(path_to_json_creds)

        self.thuc_id        = kwargs.get("thuc_id", None)

        self.outlet_x       = kwargs.get("outlet_x", None) # In EPSG 4326
        self.outlet_y       = kwargs.get("outlet_y", None)

        self.start_date     = kwargs.get("start_date", None)
        self.end_date       = kwargs.get("end_date", None)

        self.watershed_name        = kwargs.get("watershed_name", "AIMS Watershed")
        self.watershed_description = kwargs.get("watershed_description", "Watershed generated by pyAGNPS and AIMS")
        self.watershed_location    = kwargs.get("watershed_location", "A Watershed in continental United States")

        self.bounds = None # GeoDataFrame with geom column representing the outline of the group of cells

        self.climate_method = kwargs.get("climate_method", "nldas2_database")

        self.secondary_climate_ids = None
        self.climate_station_points = None

        # Placeholder for static files
        self.nldas2_centroids = None
        self.scs_storm_types  = None
        self.precip_zones     = None

        # CMIP files

        self.output_folder = Path(kwargs.get("output_folder", "/tmp/"))
        self.export_gis    = kwargs.get("export_gis", True)
        self.MAXITER_CLIMATE_QUERY = kwargs.get("MAXITER_CLIMATE_QUERY", 10)

        # Geometries
        # self.cells_geometry = None

        # self.reaches_geometry = None

        # # Data Sections
        # self.df_cells = None
        # self.cells_list = []

        # self.df_reaches = None
        # self.reaches_list = []

        # self.df_soil_sata = None
        # self.soil_ids_list = []


    def set_metadata(self, **kwargs):
        """ Defines the metadata for the watershed. """

        self.outlet_x = kwargs.get("outlet_x", self.outlet_x)
        self.outlet_y = kwargs.get("outlet_y", self.outlet_y)

        self.start_date = kwargs.get("start_date", self.start_date)
        self.end_date   = kwargs.get("end_date", self.end_date)

        self.watershed_name        = kwargs.get("watershed_name", self.watershed_name)
        self.watershed_description = kwargs.get("watershed_description", self.watershed_description)
        self.watershed_location    = kwargs.get("watershed_location", self.watershed_location)

        self.output_folder = Path(kwargs.get("output_folder", self.output_folder))
        self.export_gis    = kwargs.get("export_gis", self.export_gis)
        self.MAXITER_CLIMATE_QUERY = kwargs.get("MAXITER_CLIMATE_QUERY", self.MAXITER_CLIMATE_QUERY)     


    def get_thuc_id_by_xy(self, x=None, y=None):
        """ Returns the thuc_id for which the coordinates belong to. """
        if x is None or y is None:
            x, y = self.outlet_x, self.outlet_y
        else:
            self.outlet_x, self.outlet_y = x, y

        sql = sql_text(
            f"SELECT thuc_near_run_id_tr({x},{y})"
        )
        thuc = pd.read_sql(sql, self.engine)
        thuc_id = thuc.iloc[0].values[0]

        self.thuc_id = thuc_id
    
    def get_outlet_xy_from_thuc_id_and_reach_id(self, thuc_id, reach_id=2):
        """ If not specified the reach_id defaults to 2, the most downstream reach.
        This function returns the x and y coordinates of centroid of the reach_id in thuc_id."""
        sql = sql_text(
            f"""SELECT ST_X(ST_Centroid(ST_Collect(ST_Transform(geom, 4326)))) AS lon, 
                ST_Y(ST_Centroid(ST_Collect(ST_Transform(geom, 4326)))) AS lat 
            FROM thuc_{thuc_id}_annagnps_reach_ids
            WHERE dn = {reach_id}
            """
        )
        df = pd.read_sql(sql, self.engine)

        x, y = df["lon"].to_list()[0], df["lat"].to_list()[0]

        self.outlet_x = x
        self.outlet_y = y

        return x, y
    
    def set_reaches_for_output(self, output_reaches=['OUTLET']):
        """ Set the output_reaches for the simulation. """
        if output_reaches:
            self.output_reaches = [reach for reach in output_reaches]

    def load_cmip5_maca_station_points_clim_id(self, path_to_cmip5_station_points_clim_id, path_to_cmip5_data_dir=None):
        """
        This function loads the cmip5 station points climate id file into a GeoDataFrame if it already exists
        and if not it will try to load the cmip5 data by reading from a master directory that contains the historical data
        but also the future data. Then it will write the file to the path_to_cmip5_station_points_clim_id"""

        path_to_cmip5_station_points_clim_id = Path(path_to_cmip5_station_points_clim_id)

        if path_to_cmip5_station_points_clim_id.exists():
            self.cmip_pts = gpd.read_file(path_to_cmip5_station_points_clim_id)
        else:
            clm = climate.ClimateAnnAGNPSCoords(coords=None,
                                                start=self.start_date,
                                                end=self.end_date)
            
            path_to_cmip5_data_dir = Path(path_to_cmip5_data_dir)
            clm.read_cmip5_maca_data(path_to_cmip5_data_dir.glob("*/*.nc"))

            lat_vals = clm.ds['lat'].values
            lon_vals = clm.ds['lon'].values - 360 # For CMIP6 and CMIP5 the longitude needs to be in the [0, 360[ range, here we bring it back in the -180, 180

            all_lon_lat_pairs = list(itertools.product(lon_vals, lat_vals))

            lon_values, lat_values = zip(*all_lon_lat_pairs)

            pts = gpd.points_from_xy(x=lon_values, y=lat_values, crs=4326)
            cmip_pts = gpd.GeoDataFrame(geometry=pts, crs=4326)

            cmip_pts['clim_id'] = cmip_pts['geometry'].apply(lambda geom: clm.generate_cmip_lon_lat_secondary_climate_id((geom.x, geom.y)))


            cmip_pts.to_file(path_to_cmip5_station_points_clim_id, driver='GPKG')

            self.cmip_pts = cmip_pts

    def load_nldas2_centroids(self, path_to_nldas2_centroids=None):
        if path_to_nldas2_centroids is None:
            path_to_nldas2_centroids = "https://amazon.ncche.olemiss.edu:8443/Luc/pyagnps/-/raw/main/inputs/climate/NLDAS2_GRID_CENTROIDS_epsg4326.gpkg"
        else:
            path_to_nldas2_centroids = Path(path_to_nldas2_centroids)

        self.nldas2_centroids = gpd.read_file(path_to_nldas2_centroids)

    def load_scs_storm_types(self, path_to_scs_storm_types=None):
        if path_to_scs_storm_types is None:
            path_to_scs_storm_types = "https://amazon.ncche.olemiss.edu:8443/Luc/rusle2-climate-shapefile/-/raw/main/data/scs_storm_types.gpkg"
        else:
            path_to_scs_storm_types = Path(path_to_scs_storm_types)

        self.scs_storm_types = gpd.read_file(path_to_scs_storm_types).to_crs("epsg:4326")

    def load_precip_zones(self, path_to_precip_zones=None):
        if path_to_precip_zones is None:
            path_to_precip_zones = "https://amazon.ncche.olemiss.edu:8443/Luc/rusle2-climate-shapefile/-/raw/main/outputs/precip_zones_RUSLE2_cleaned_manually_extrapolated_pchip_linear_US_units.gpkg"
        else:
            path_to_precip_zones = Path(path_to_precip_zones)

        self.precip_zones = gpd.read_file(path_to_precip_zones)

    def load_static_files(self, **kwargs):

        path_to_nldas2_centroids = kwargs.get("path_to_nldas2_centroids", None)
        path_to_scs_storm_types = kwargs.get("path_to_scs_storm_types", None)
        path_to_precip_zones = kwargs.get("path_to_precip_zones", None)

        self.load_nldas2_centroids(path_to_nldas2_centroids=path_to_nldas2_centroids)
        self.load_scs_storm_types(path_to_scs_storm_types=path_to_scs_storm_types)
        self.load_precip_zones(path_to_precip_zones=path_to_precip_zones)

    def make_watershed_input_dirs(self):

        subdirs = ['general', 'climate', 'simulation', 'watershed']
        subdirs.extend(['GIS'] if self.export_gis else [])
        
        folder_paths = annagnps.make_annagnps_inputs_dirs(self.output_folder, subdirs=subdirs)
        
        input_folders = {
            dir_name: path 
            for dir_name, path in zip(subdirs, folder_paths)
        }

        self.input_folders = input_folders

        return input_folders
    
    def query_cells(self):

        if self.thuc_id is None:
            self.get_thuc_id_by_xy()
        
        thuc_id = self.thuc_id
        lon, lat = self.outlet_x, self.outlet_y

        # Geometry of cells
        cells_query = f"SELECT geom, cell_id FROM thuc_cell_geo_tr({lon},{lat}, '{thuc_id}')"

        cells_geometry = gpd.read_postgis(sql=sql_text(cells_query), con=self.engine.connect(), geom_col='geom')
        cells_geometry = cells_geometry.dissolve(by='cell_id').reset_index()

        cells_list = cells_geometry['cell_id'].unique() # List of cell_ids
        
        # Cells data section
        cells_string = ", ".join(map(str, cells_list.tolist()))
        cells_data_section = f"SELECT * FROM thuc_{thuc_id}_annagnps_cell_data_section WHERE cell_id in ({cells_string})"

        df_cells = pd.read_sql_query(sql=sql_text(cells_data_section), con=self.engine.connect())

        columns = [col for col in df_cells.columns if col not in ['soil_id_annagnps_valid']]

        df_cells = df_cells[columns]

        cells_geometry = cells_geometry.merge(df_cells, on='cell_id')

        self.df_cells = df_cells

        self.cells_geometry = cells_geometry
        self.cells_list = cells_list

    def query_reaches(self):

        if self.thuc_id is None:
            self.get_thuc_id_by_xy()

        thuc_id = self.thuc_id
        lon, lat = self.outlet_x, self.outlet_y

        reaches_query = f"SELECT geom, reach_id FROM thuc_reach_geo_tr({lon},{lat}, '{thuc_id}')"

        reaches_geometry = gpd.read_postgis(sql=sql_text(reaches_query), con=self.engine.connect(), geom_col='geom')
        reaches_geometry = reaches_geometry.dissolve(by='reach_id').reset_index()

        reaches_list = reaches_geometry['reach_id'].unique() # List of reach_ids
        reaches_string = ", ".join(map(str, reaches_list.tolist()))

        # Reaches data section
        reaches_data_section = f"SELECT * FROM thuc_{thuc_id}_annagnps_reach_data_section WHERE reach_id in ({reaches_string})"

        df_reaches = pd.read_sql_query(sql=sql_text(reaches_data_section), con=self.engine.connect())

        reaches_geometry = reaches_geometry.merge(df_reaches, on='reach_id')

        df_reaches_valid = annagnps.make_df_reaches_valid(df_reaches)

        self.df_reaches = df_reaches_valid

        self.reaches_geometry = reaches_geometry
        self.reaches_list = reaches_list
    
    def query_soil(self):

        soil_ids_list = self.df_cells['soil_id'].unique()
        soil_ids_string = ", ".join(map(str, soil_ids_list.tolist()))

        query_soil = f"""SELECT * FROM usa_valid_soil_data WHERE "Soil_ID" in ({soil_ids_string})"""
        query_soil_layers = f"""SELECT * FROM usa_valid_soil_layers_data WHERE "Soil_ID" in ({soil_ids_string})"""
        query_raw = f"""SELECT * FROM raw_nrcs_soil_data WHERE "mukey" in ({soil_ids_string})"""

        df_soil_data = pd.read_sql_query(sql=sql_text(query_soil), con=self.engine.connect())
        df_soil_layers_data = pd.read_sql_query(sql=sql_text(query_soil_layers), con=self.engine.connect())\
                            .sort_values(by=['Soil_ID','Layer_Number'])

        df_raw = pd.read_sql_query(sql=sql_text(query_raw), con=self.engine.connect())

        self.soil_ids_list = soil_ids_list
        self.df_soil_data = df_soil_data
        self.df_soil_layers_data = df_soil_layers_data
        self.df_raw = df_raw

    def query_management_field(self):

        mgmt_field_ids_list = self.df_cells['mgmt_field_id'].unique()
        mgmt_field_ids_string = ", ".join(f"'{m}'" for m in mgmt_field_ids_list)

        query_mgmt_field_data = f"""SELECT * FROM annagnps_mgmt_field WHERE "Field_ID" in ({mgmt_field_ids_string})"""
        df_mgmt_field = pd.read_sql_query(sql=sql_text(query_mgmt_field_data), con=self.engine.connect())

        self.df_mgmt_field = df_mgmt_field
        self.df_mgmt_field_ids_list = mgmt_field_ids_list

    def query_management_schedule(self):

        mgmt_schedule_ids_list = self.df_mgmt_field['Mgmt_Schd_ID'].unique()
        mgmt_schedule_ids_string = ", ".join(f"'{m}'" for m in mgmt_schedule_ids_list)

        query_mgmt_schd_data = f"""SELECT * FROM annagnps_mgmt_schd WHERE "Mgmt_Schd_ID" in ({mgmt_schedule_ids_string})"""
        df_mgmt_schd = pd.read_sql_query(sql=sql_text(query_mgmt_schd_data), con=self.engine.connect())

        self.df_mgmt_schd = df_mgmt_schd
        self.df_mgmt_schedule_ids_list = mgmt_schedule_ids_list

    def query_management_crop(self):

        mgmt_crop_ids_list = self.df_mgmt_schd['New_Crop_ID'].dropna().unique()
        mgmt_crop_ids_string = ", ".join(f"'{m}'" for m in mgmt_crop_ids_list)

        query_mgmt_schd_data = f"""SELECT * FROM annagnps_crop WHERE "Crop_ID" in ({mgmt_crop_ids_string})"""
        df_mgmt_crop = pd.read_sql_query(sql=sql_text(query_mgmt_schd_data), con=self.engine.connect())

        self.df_mgmt_crop = df_mgmt_crop
        self.df_mgmt_crop_ids_list = mgmt_crop_ids_list

    def query_management_crop_growth(self):

        mgmt_crop_ids_list = self.df_mgmt_schd['New_Crop_ID'].dropna().unique()
        mgmt_crop_ids_string = ", ".join(f"'{m}'" for m in mgmt_crop_ids_list)

        query_mgmt_crop_growth_data = f"""SELECT * FROM annagnps_crop_growth WHERE "Crop_Growth_ID" in ({mgmt_crop_ids_string})"""
        df_mgmt_crop_growth = pd.read_sql_query(sql=sql_text(query_mgmt_crop_growth_data), con=self.engine.connect())

        self.df_mgmt_crop_growth = df_mgmt_crop_growth
        self.mgmt_crop_ids_list = mgmt_crop_ids_list

    def query_management_non_crop(self):

        mgmt_non_crop_ids_list = self.df_mgmt_schd['New_Non-Crop_ID'].dropna().unique()
        mgmt_non_crop_ids_string = ", ".join(f"'{m}'" for m in mgmt_non_crop_ids_list)

        query_mgmt_non_cropdata = f"""SELECT * FROM annagnps_non_crop WHERE "Non-Crop_ID" in ({mgmt_non_crop_ids_string})"""
        df_mgmt_non_crop = pd.read_sql_query(sql=sql_text(query_mgmt_non_cropdata), con=self.engine.connect())

        self.df_mgmt_non_crop = df_mgmt_non_crop
        self.df_mgmt_non_crop_ids_list = mgmt_non_crop_ids_list

    def query_management_operation(self):

        mgmt_oper_ids_list = self.df_mgmt_schd['Mgmt_Operation_ID'].dropna().unique()
        mgmt_oper_ids_string = ", ".join(f"'{m}'" for m in mgmt_oper_ids_list)

        query_mgmt_oper_data = f"""SELECT * FROM annagnps_mgmt_oper WHERE "Mgmt_Operation_ID" in ({mgmt_oper_ids_string})"""

        df_mgmt_oper = pd.read_sql_query(sql=sql_text(query_mgmt_oper_data), con=self.engine.connect())

        self.df_mgmt_oper = df_mgmt_oper
        self.df_mgmt_oper_ids_list = mgmt_oper_ids_list

    def query_runoff_curve(self):

        roc_ids_list = self.df_mgmt_schd['Curve_Number_ID'].dropna().unique()
        roc_ids_string = ", ".join(f"'{m}'" for m in roc_ids_list)

        query_roc_data = f"""SELECT * FROM annagnps_runoff_curve WHERE "Curve_Number_ID" in ({roc_ids_string})"""
        df_roc = pd.read_sql_query(sql=sql_text(query_roc_data), con=self.engine.connect())

        self.df_roc = df_roc
        self.df_roc_ids_list = roc_ids_list

    def get_watershed_bounds(self, cells_geometry=None, save=True):
        """
        Get the watershed outline from the cells geometry.

        If save is True, the bounds will be saved to the Watershed dataframe
        """

        if cells_geometry is None:
            cells_geometry = self.cells_geometry

            bounds = utils.get_bounds_from_cells(self.cells_geometry)

        else:

            bounds = utils.get_bounds_from_cells(cells_geometry)

        if save:
            self.bounds = bounds

        return bounds
    
    def get_dominant_storm_type(self, cells_geometry=None):

        if self.scs_storm_types is None:
            self.load_scs_storm_types()

        scs_storm_types = self.scs_storm_types

        # Whole watershed
        if cells_geometry is None:
            cells_geometry = self.cells_geometry

            if self.bounds is None:
                self.get_watershed_bounds(save=True)
                bounds = self.bounds

        # Watershed bounds specified by cells_geometry
        else:
            bounds = self.get_watershed_bounds(cells_geometry=cells_geometry, save=False)

        bounds_scs = bounds.overlay(scs_storm_types)
        bounds_scs['area'] = bounds_scs.geometry.area

        main_storm_type = bounds_scs.loc[bounds_scs['area'].argmax(), 'SCS Zone Type']

        return main_storm_type
    
    def get_weighted_precip_zones_parameters(self, cells_geometry=None):
        """
        Uses the static file representing the precip zone parameters R_factor and 10_year_EI and dominant EI. 
        The values are weighted by the area of the cells.
        """

        if self.precip_zones is None:
            self.load_precip_zones()

        if cells_geometry is None:
            cells_geometry = self.cells_geometry

            if self.bounds is None:
                self.get_watershed_bounds(save=True)
                bounds = self.bounds
        # Watershed bounds specified by cells_geometry
        else:
            bounds = self.get_watershed_bounds(cells_geometry=cells_geometry, save=False)

        bounds_precip = bounds.overlay(self.precip_zones)
        bounds_precip['area'] = bounds_precip.geometry.area

        weighted_R_fctr = (bounds_precip['area'] * bounds_precip['R_factor']).sum() / bounds_precip['area'].sum()
        weighted_10_year_EI = (bounds_precip['area'] * bounds_precip['10_year_EI']).sum() / bounds_precip['area'].sum()

        dominant_EI = bounds_precip.loc[bounds_precip['area'].argmax(), 'EI_Zone']

        if dominant_EI == 'default':
            dominant_EI = constants.DEFAULT_EI_NUMBER # 100 
        else:
            dominant_EI = int(dominant_EI.replace('US_',''))

        return weighted_R_fctr, weighted_10_year_EI, dominant_EI
    
    def get_watershed_centroid_xy(self):

        if self.bounds is None:
                self.get_watershed_bounds(save=True)
                bounds = self.bounds
        else:
            bounds = self.bounds

        watershed_centroid = bounds.centroid
        x0, y0 = watershed_centroid.x[0], watershed_centroid.centroid.y[0] 

        return x0, y0
    
    def reset_watershed_secondary_climate_ids(self):
        self.cells_geometry = self.cells_geometry.assign(secondary_climate_file_id=None)

    def identify_secondary_climate_ids(self):
        """
        Identifies the secondary climate file ids for each cell and
        
        Returns a DataFrame with a an index called "cell_id" and a column called "secondary_climate_file_id" and a "geometry'
        column that is the location of the climate station
        """
        if self.climate_method in ['nldas2', 'nldas2_database', 'nldas2_data_rods']:
            column_station_id_name = 'nldas2_grid_ID'

            if self.nldas2_centroids is None:
                self.load_nldas2_centroids()

            gdf_station_points = self.nldas2_centroids

        elif self.climate_method == 'cmip5':
            column_station_id_name = 'clim_id'

            gdf_station_points = self.cmip_pts


        cells_geometry = self.cells_geometry
        cells_geometry = cells_geometry.sjoin_nearest(gdf_station_points)
        # Clean-up the spatial join
        cells_geometry['secondary_climate_file_id'] = cells_geometry[column_station_id_name]
        cells_geometry.drop(columns=[column_station_id_name, 'index_right'], inplace=True)

        secondary_climate_ids = cells_geometry.loc[:,['cell_id', 'secondary_climate_file_id']]\
                                .drop_duplicates(subset='cell_id')#.set_index('cell_id')
        
        secondary_climate_ids = gdf_station_points.merge(secondary_climate_ids, left_on=column_station_id_name, right_on='secondary_climate_file_id')
        secondary_climate_ids.drop(columns=[column_station_id_name], inplace=True)

        secondary_climate_ids = secondary_climate_ids.set_index('cell_id')
        secondary_climate_ids = secondary_climate_ids.to_crs('epsg:4326')

        self.secondary_climate_ids = secondary_climate_ids

    def update_cell_data_section_with_secondary_climate_ids(self):
        # if self.secondary_climate_ids is None:
        #     if 'nldas2' in self.climate_method:
        #         self.nldas2_identify_secondary_climate_ids()

        df_cells = self.df_cells

        if df_cells.index.name != 'cell_id':
            df_cells = df_cells.set_index('cell_id')

        df_cells.update(self.secondary_climate_ids[['secondary_climate_file_id']])
        df_cells = df_cells.reset_index()

        self.df_cells = df_cells

    def generate_climate_daily_files(self, **kwargs):

        date_mode = kwargs.get("date_mode", "local")
        overwrite = kwargs.get("overwrite", True)
        table     = kwargs.get("table", "climate_nldas2")
        
        self.reset_watershed_secondary_climate_ids()

        # Create a placeholder climate object
        clm = climate.ClimateAnnAGNPSCoords(coords=None,
                                            start=self.start_date,
                                            end=self.end_date)
        

        if 'cmip5' in self.climate_method: # CHECK THE LOGIC HERE FOR WHEN cmip_pts is loaded. ADD kwargs to identify function
            path_to_cmip_dir = kwargs.get("path_to_cmip_dir", None)
            path_to_cmip_raster_points_clim_id = kwargs.get("path_to_cmip_station_points_id", None)

            if path_to_cmip_raster_points_clim_id is None:
                path_to_cmip_raster_points_clim_id = path_to_cmip_dir / "cmip5_maca_v2_metdata_pts_clim_ids.gpkg"

            self.load_cmip5_maca_station_points_clim_id(path_to_cmip_raster_points_clim_id, 
                                                        path_to_cmip5_data_dir=path_to_cmip_dir)            

            clm.read_cmip5_maca_data(path_to_cmip_dir.glob("*/*.nc"))

        self.identify_secondary_climate_ids()
        self.update_cell_data_section_with_secondary_climate_ids()

        climate_station_points = self.secondary_climate_ids

        # Unique rows
        climate_station_points = climate_station_points.drop_duplicates(subset=['secondary_climate_file_id'])
        
        climate_dir = self.input_folders['climate']

        for feature in tqdm(climate_station_points.iterfeatures(), total=len(climate_station_points)):

            x, y    = feature['geometry']['coordinates']
            clim_id = feature['properties']['secondary_climate_file_id']

            if self.climate_method == 'nldas2_data_rods':
                
                climate_station_name = f"NLDAS-2 Grid ID {clim_id}. Queried with Hydrology Data Rods."

                clm = climate.ClimateAnnAGNPSCoords(coords=(x,y), 
                                                    start=self.start_date, end=self.end_date, 
                                                    date_mode=date_mode)
                df = clm.query_nldas2_generate_annagnps_climate_daily()
                # implement with data rods query
            elif self.climate_method == 'nldas2_database':

                
                climate_station_name = f"NLDAS-2 Grid ID {clim_id}. From AIMS Database."

                df = climate.query_annagnps_climate_timeseries_db(station_id=clim_id,
                                                                  engine=self.engine,
                                                                  table=table,
                                                                  start_date=self.start_date,
                                                                  end_date=self.end_date)

            elif self.climate_method == 'cmip5':

                climate_station_name = f"CMIP5 MACAv2METDATA raster ID {clim_id}"
                
                # THIS clm NEEDS TO COME FROM SOMEWHERE!
                clm.update_coords_start_end_dates(coords=(x,y), 
                                                  start=self.start_date, end=self.end_date, 
                                                  date_mode=date_mode)
                df = clm.generate_annagnps_daily_climate_data_cmip5_maca()
                pass
                # implement with CMIP5 static files

            # Store climate station data
            climate_station = {
                # 'data': df, # DataFrame containing the climate data
                'climate_station': { # Climate station metadata
                    'output_filepath': climate_dir / f'climate_station_{clim_id}.csv',
                    'climate_station_name': climate_station_name,
                    'beginning_climate_date': clm.start.strftime("%m/%d/%Y"),
                    'ending_climate_date': clm.end.strftime("%m/%d/%Y"),
                    'latitude': y,
                    'longitude': x,
                    'elevation': self.cells_geometry.loc[self.cells_geometry['secondary_climate_file_id'] == clim_id, 'avg_elevation'].mean()
                    }
            }

            # Write climate_daily file
            climate_path = climate_dir / f"climate_daily_{clim_id}.csv"

            if not(climate_path.exists()) or overwrite:
                df.to_csv(climate_path, index=False, float_format="%.3f")
                # Write climate_station file
                climate.generate_climate_station_file(**climate_station['climate_station'])

        # After the loop copy the last climate station to have one global station for AnnAGNPS to run
        # Even if it won't be used

        # Write climate_daily file
        global_climate_path = climate_dir / f"climate_daily.csv"

        if not(global_climate_path.exists()) or overwrite:
            df.to_csv(global_climate_path, index=False, float_format="%.3f")

        global_station_path = climate_dir / f'climate_station.csv'

        if not(global_station_path.exists()) or overwrite:
            global_station = copy.deepcopy(climate_station['climate_station'])
            global_station['output_filepath'] = global_station_path
            climate.generate_climate_station_file(**global_station)
        

def open_creds_dict(path_to_json_creds):
    with open(path_to_json_creds, "r") as f:
        credentials = json.load(f)
        return credentials
    
def create_db_url_object(credentials):
    url_object = URL.create(
        "postgresql",
        username=credentials["user"],
        password=credentials["password"],
        host=credentials["host"],
        port=credentials["port"],
        database=credentials["database"],
    )
    return url_object