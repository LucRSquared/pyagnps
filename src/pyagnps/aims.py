# COLLECTION OF AIMS SPECIFIC SCRIPTS AND METHODS
# This assumes that a connection to a database with properly configured
# tables for thucs is available

from pathlib import Path

from datetime import datetime

import itertools

import copy
from multiprocessing.pool import ThreadPool
from functools import partial

import json

import pandas as pd
import geopandas as gpd

from sqlalchemy import create_engine, text as sql_text
from sqlalchemy import URL

from pyagnps import annagnps, utils, constants, climate
from pyagnps.utils import relative_input_file_path, write_csv_control_file_from_dict

from tqdm import tqdm

class AIMSWatershed:
    def __init__(self, path_to_json_creds, **kwargs):
        credentials = open_creds_dict(path_to_json_creds)
        url_object = create_db_url_object(credentials)

        self.engine = create_engine(url_object)
        self.path_to_json_creds = Path(path_to_json_creds)

        self.thuc_id        = kwargs.get("thuc_id", None)
        self.reach_id       = kwargs.get("reach_id", None)

        self.outlet_x       = kwargs.get("outlet_x", None) # In EPSG 4326
        self.outlet_y       = kwargs.get("outlet_y", None)

        self._check_outlet_coords()

        self.start_date     = kwargs.get("start_date", None)
        self.end_date       = kwargs.get("end_date", None)

        self.watershed_name        = kwargs.get("watershed_name", "AIMS Watershed")
        self.watershed_description = kwargs.get("watershed_description", "Watershed generated by pyAGNPS and AIMS")
        self.watershed_location    = kwargs.get("watershed_location", "A Watershed in continental United States")

        self.bounds = None # GeoDataFrame with geom column representing the outline of the group of cells

        self.climate_method = kwargs.get("climate_method", "nldas2_database")
        self.date_mode      = kwargs.get("date_mode", "local")

        # Database climate table name
        self.climate_table = kwargs.get("climate_table", "climate_nldas2")

        # Paths for CMIP data
        self.path_to_cmip_dir               = kwargs.get("path_to_cmip_dir", None)
        self.path_to_cmip_station_points_id = kwargs.get("path_to_cmip_station_points_id", None)

        self.secondary_climate_ids  = None
        self.climate_station_points = None

        # Placeholder for static files
        self.path_to_nldas2_centroids = kwargs.get("path_to_nldas2_centroids", None)
        self.path_to_scs_storm_types  = kwargs.get("path_to_scs_storm_types", None)
        self.path_to_precip_zones     = kwargs.get("path_to_precip_zones", None)

        self.nldas2_centroids = None
        self.scs_storm_types  = None
        self.precip_zones     = None

        # CMIP files

        self.output_folder = Path(kwargs.get("output_folder", f"/tmp/watershed_x_{self.outlet_x}_y_{self.outlet_y}_{get_now_string()}"))
        self.export_gis    = kwargs.get("export_gis", True)
        self.overwrite     = kwargs.get("overwrite", True)
        self.MAXITER_CLIMATE_QUERY = kwargs.get("MAXITER_CLIMATE_QUERY", 10)

        self.selected_reaches_for_output = kwargs.get("selected_reaches_for_output", None)
        self.set_reaches_for_output(output_reaches=self.selected_reaches_for_output)

        # Geometries
        # self.cells_geometry = None

        # self.reaches_geometry = None

        # # Data Sections
        # self.df_cells = None
        # self.cells_list = []

        # self.df_reaches = None
        # self.reaches_list = []

        # self.df_soil_sata = None
        # self.soil_ids_list = []


    def set_metadata(self, **kwargs):
        """ Defines the metadata for the watershed. """

        self.outlet_x = kwargs.get("outlet_x", self.outlet_x)
        self.outlet_y = kwargs.get("outlet_y", self.outlet_y)

        self.thuc_id  = kwargs.get("thuc_id", self.thuc_id)
        self.reach_id = kwargs.get("reach_id", self.reach_id)

        self.start_date = kwargs.get("start_date", self.start_date)
        self.end_date   = kwargs.get("end_date", self.end_date)

        self.watershed_name        = kwargs.get("watershed_name", self.watershed_name)
        self.watershed_description = kwargs.get("watershed_description", self.watershed_description)
        self.watershed_location    = kwargs.get("watershed_location", self.watershed_location)

        self.path_to_cmip_dir = kwargs.get("path_to_cmip_dir", self.path_to_cmip_dir)
        self.path_to_cmip_station_points_id = kwargs.get("path_to_cmip_station_points_id", self.path_to_cmip_station_points_id)

        self.climate_table = kwargs.get("climate_table", self.climate_table)
        self.date_mode     = kwargs.get("date_mode", self.date_mode)

        self.path_to_nldas2_centroids = kwargs.get("path_to_nldas2_centroids", self.path_to_nldas2_centroids)
        self.path_to_scs_storm_types  = kwargs.get("path_to_scs_storm_types", self.path_to_scs_storm_types)
        self.path_to_precip_zones     = kwargs.get("path_to_precip_zones", self.path_to_precip_zones)

        self.output_folder    = Path(kwargs.get("output_folder", self.output_folder))
        self.export_gis            = kwargs.get("export_gis", self.export_gis)
        self.overwrite              = kwargs.get("overwrite", self.overwrite)
        self.MAXITER_CLIMATE_QUERY = kwargs.get("MAXITER_CLIMATE_QUERY", self.MAXITER_CLIMATE_QUERY)

        self.selected_reaches_for_output = kwargs.get("selected_reaches_for_output", self.selected_reaches_for_output)
        self.set_reaches_for_output(output_reaches=self.selected_reaches_for_output)

        self._check_outlet_coords()   


    def get_thuc_id_by_xy(self, x=None, y=None):
        """ Returns the thuc_id for which the coordinates belong to. """
        if x is None or y is None:
            x, y = self.outlet_x, self.outlet_y
        else:
            self.outlet_x, self.outlet_y = x, y

        sql = sql_text(
            f"SELECT thuc_near_run_id_tr({x},{y})"
        )
        thuc = pd.read_sql(sql, self.engine)
        thuc_id = thuc.iloc[0].values[0]

        self.thuc_id = thuc_id

    def _check_outlet_coords(self):
        """
        This function makes sure that outlet_x and outlet_y are set.
        If thuc_id and reach_id are provided they take priority"
        """
        if self.outlet_x is None or self.outlet_y is None:
            if self.thuc_id and self.reach_id:
                self.get_outlet_xy_from_thuc_id_and_reach_id(self.thuc_id, self.reach_id)
            else:
                raise Exception("You must set outlet_x and outlet_y or thuc_id and reach_id.")
    
    def get_outlet_xy_from_thuc_id_and_reach_id(self, thuc_id, reach_id=2):
        """ If not specified the reach_id defaults to 2, the most downstream reach.
        This function returns the x and y coordinates of centroid of the reach_id in thuc_id."""
        sql = sql_text(
            f"""SELECT ST_X(ST_Centroid(ST_Collect(ST_Transform(geom, 4326)))) AS lon, 
                ST_Y(ST_Centroid(ST_Collect(ST_Transform(geom, 4326)))) AS lat 
            FROM thuc_{thuc_id}_annagnps_reach_ids
            WHERE dn = {reach_id}
            """
        )
        df = pd.read_sql(sql, self.engine)

        x, y = df["lon"].to_list()[0], df["lat"].to_list()[0]

        self.outlet_x = x
        self.outlet_y = y

        return x, y
    
    def set_reaches_for_output(self, output_reaches=['OUTLET']):
        """ Set the output_reaches for the simulation. """
        if output_reaches:
            self.output_reaches = [reach for reach in output_reaches]

    def load_cmip5_maca_station_points_clim_id(self, path_to_cmip5_station_points_clim_id, path_to_cmip5_data_dir=None):
        """
        This function loads the cmip5 station points climate id file into a GeoDataFrame if it already exists
        and if not it will try to load the cmip5 data by reading from a master directory that contains the historical data
        but also the future data. Then it will write the file to the path_to_cmip5_station_points_clim_id"""

        path_to_cmip5_station_points_clim_id = Path(path_to_cmip5_station_points_clim_id)

        if path_to_cmip5_station_points_clim_id.exists():
            self.cmip_pts = gpd.read_file(path_to_cmip5_station_points_clim_id)
        else:
            clm = climate.ClimateAnnAGNPSCoords(coords=None,
                                                start=self.start_date,
                                                end=self.end_date)
            
            path_to_cmip5_data_dir = Path(path_to_cmip5_data_dir)
            clm.read_cmip5_maca_data(path_to_cmip5_data_dir.glob("*/*.nc"))

            lat_vals = clm.ds['lat'].values
            lon_vals = clm.ds['lon'].values - 360 # For CMIP6 and CMIP5 the longitude needs to be in the [0, 360[ range, here we bring it back in the -180, 180

            all_lon_lat_pairs = list(itertools.product(lon_vals, lat_vals))

            lon_values, lat_values = zip(*all_lon_lat_pairs)

            pts = gpd.points_from_xy(x=lon_values, y=lat_values, crs=4326)
            cmip_pts = gpd.GeoDataFrame(geometry=pts, crs=4326)

            cmip_pts['clim_id'] = cmip_pts['geometry'].apply(lambda geom: clm.generate_cmip_lon_lat_secondary_climate_id((geom.x, geom.y)))


            cmip_pts.to_file(path_to_cmip5_station_points_clim_id, driver='GPKG')

            self.cmip_pts = cmip_pts

    def load_nldas2_centroids(self, path_to_nldas2_centroids=None):
        if path_to_nldas2_centroids is None:
            path_to_nldas2_centroids = "https://amazon.ncche.olemiss.edu:8443/Luc/pyagnps/-/raw/main/inputs/climate/NLDAS2_GRID_CENTROIDS_epsg4326.gpkg"
        else:
            path_to_nldas2_centroids = Path(path_to_nldas2_centroids)

        self.nldas2_centroids = gpd.read_file(path_to_nldas2_centroids)

    def load_scs_storm_types(self, path_to_scs_storm_types=None):
        if path_to_scs_storm_types is None:
            path_to_scs_storm_types = "https://amazon.ncche.olemiss.edu:8443/Luc/rusle2-climate-shapefile/-/raw/main/data/scs_storm_types.gpkg"
        else:
            path_to_scs_storm_types = Path(path_to_scs_storm_types)

        self.scs_storm_types = gpd.read_file(path_to_scs_storm_types).to_crs("epsg:4326")

    def load_precip_zones(self, path_to_precip_zones=None):
        if path_to_precip_zones is None:
            path_to_precip_zones = "https://amazon.ncche.olemiss.edu:8443/Luc/rusle2-climate-shapefile/-/raw/main/outputs/precip_zones_RUSLE2_cleaned_manually_extrapolated_pchip_linear_US_units.gpkg"
        else:
            path_to_precip_zones = Path(path_to_precip_zones)

        self.precip_zones = gpd.read_file(path_to_precip_zones)

    def load_static_files(self, **kwargs):

        path_to_nldas2_centroids = kwargs.get("path_to_nldas2_centroids", self.path_to_nldas2_centroids)
        path_to_scs_storm_types = kwargs.get("path_to_scs_storm_types", self.path_to_scs_storm_types)
        path_to_precip_zones = kwargs.get("path_to_precip_zones", self.path_to_precip_zones)

        self.load_nldas2_centroids(path_to_nldas2_centroids=path_to_nldas2_centroids)
        self.load_scs_storm_types(path_to_scs_storm_types=path_to_scs_storm_types)
        self.load_precip_zones(path_to_precip_zones=path_to_precip_zones)

    def make_watershed_input_dirs(self):

        subdirs = ['general', 'climate', 'simulation', 'watershed']
        subdirs.extend(['GIS'] if self.export_gis else [])
        
        folder_paths = annagnps.make_annagnps_inputs_dirs(self.output_folder, subdirs=subdirs)
        
        input_folders = {
            dir_name: path 
            for dir_name, path in zip(subdirs, folder_paths)
        }

        self.input_folders = input_folders

        return input_folders
    
    def query_cells(self):

        if self.thuc_id is None:
            self.get_thuc_id_by_xy()
        
        thuc_id = self.thuc_id
        lon, lat = self.outlet_x, self.outlet_y

        # Geometry of cells
        cells_query = f"SELECT geom, cell_id FROM thuc_cell_geo_tr({lon},{lat}, '{thuc_id}')"

        cells_geometry = gpd.read_postgis(sql=sql_text(cells_query), con=self.engine.connect(), geom_col='geom')
        cells_geometry = cells_geometry.dissolve(by='cell_id').reset_index()

        cells_list = cells_geometry['cell_id'].unique() # List of cell_ids
        
        # Cells data section
        cells_string = ", ".join(map(str, cells_list.tolist()))
        cells_data_section = f"SELECT * FROM thuc_{thuc_id}_annagnps_cell_data_section WHERE cell_id in ({cells_string})"

        df_cells = pd.read_sql_query(sql=sql_text(cells_data_section), con=self.engine.connect())

        columns = [col for col in df_cells.columns if col not in ['soil_id_annagnps_valid']]

        df_cells = df_cells[columns]

        cells_geometry = cells_geometry.merge(df_cells, on='cell_id')

        self.df_cells = df_cells

        self.cells_geometry = cells_geometry
        self.cells_list = cells_list

    def query_reaches(self):

        if self.thuc_id is None:
            self.get_thuc_id_by_xy()

        thuc_id = self.thuc_id
        lon, lat = self.outlet_x, self.outlet_y

        reaches_query = f"SELECT geom, reach_id FROM thuc_reach_geo_tr({lon},{lat}, '{thuc_id}')"

        reaches_geometry = gpd.read_postgis(sql=sql_text(reaches_query), con=self.engine.connect(), geom_col='geom')
        reaches_geometry = reaches_geometry.dissolve(by='reach_id').reset_index()

        reaches_list = reaches_geometry['reach_id'].unique() # List of reach_ids
        reaches_string = ", ".join(map(str, reaches_list.tolist()))

        # Reaches data section
        reaches_data_section = f"SELECT * FROM thuc_{thuc_id}_annagnps_reach_data_section WHERE reach_id in ({reaches_string})"

        df_reaches = pd.read_sql_query(sql=sql_text(reaches_data_section), con=self.engine.connect())

        reaches_geometry = reaches_geometry.merge(df_reaches, on='reach_id')

        df_reaches_valid = annagnps.make_df_reaches_valid(df_reaches)

        self.df_reaches = df_reaches_valid

        self.reaches_geometry = reaches_geometry
        self.reaches_list = reaches_list
    
    def query_soil(self):

        soil_ids_list = self.df_cells['soil_id'].unique()
        soil_ids_string = ", ".join(map(str, soil_ids_list.tolist()))

        query_soil = f"""SELECT * FROM usa_valid_soil_data WHERE "Soil_ID" in ({soil_ids_string})"""
        query_soil_layers = f"""SELECT * FROM usa_valid_soil_layers_data WHERE "Soil_ID" in ({soil_ids_string})"""
        query_raw = f"""SELECT * FROM raw_nrcs_soil_data WHERE "mukey" in ({soil_ids_string})"""

        df_soil_data = pd.read_sql_query(sql=sql_text(query_soil), con=self.engine.connect())
        df_soil_layers_data = pd.read_sql_query(sql=sql_text(query_soil_layers), con=self.engine.connect())\
                            .sort_values(by=['Soil_ID','Layer_Number'])

        df_raw = pd.read_sql_query(sql=sql_text(query_raw), con=self.engine.connect())

        self.soil_ids_list = soil_ids_list
        self.df_soil_data = df_soil_data
        self.df_soil_layers_data = df_soil_layers_data
        self.df_raw = df_raw

    def query_management_field(self):

        mgmt_field_ids_list = self.df_cells['mgmt_field_id'].unique()
        mgmt_field_ids_string = ", ".join(f"'{m}'" for m in mgmt_field_ids_list)

        query_mgmt_field_data = f"""SELECT * FROM annagnps_mgmt_field WHERE "Field_ID" in ({mgmt_field_ids_string})"""
        df_mgmt_field = pd.read_sql_query(sql=sql_text(query_mgmt_field_data), con=self.engine.connect())

        self.df_mgmt_field = df_mgmt_field
        self.df_mgmt_field_ids_list = mgmt_field_ids_list

    def query_management_schedule(self):

        mgmt_schedule_ids_list = self.df_mgmt_field['Mgmt_Schd_ID'].unique()
        mgmt_schedule_ids_string = ", ".join(f"'{m}'" for m in mgmt_schedule_ids_list)

        query_mgmt_schd_data = f"""SELECT * FROM annagnps_mgmt_schd WHERE "Mgmt_Schd_ID" in ({mgmt_schedule_ids_string})"""
        df_mgmt_schd = pd.read_sql_query(sql=sql_text(query_mgmt_schd_data), con=self.engine.connect())

        self.df_mgmt_schd = df_mgmt_schd
        self.df_mgmt_schedule_ids_list = mgmt_schedule_ids_list

    def query_management_crop(self):

        mgmt_crop_ids_list = self.df_mgmt_schd['New_Crop_ID'].dropna().unique()
        mgmt_crop_ids_string = ", ".join(f"'{m}'" for m in mgmt_crop_ids_list)

        query_mgmt_schd_data = f"""SELECT * FROM annagnps_crop WHERE "Crop_ID" in ({mgmt_crop_ids_string})"""
        df_mgmt_crop = pd.read_sql_query(sql=sql_text(query_mgmt_schd_data), con=self.engine.connect())

        self.df_mgmt_crop = df_mgmt_crop
        self.df_mgmt_crop_ids_list = mgmt_crop_ids_list

    def query_management_crop_growth(self):

        mgmt_crop_ids_list = self.df_mgmt_schd['New_Crop_ID'].dropna().unique()
        mgmt_crop_ids_string = ", ".join(f"'{m}'" for m in mgmt_crop_ids_list)

        query_mgmt_crop_growth_data = f"""SELECT * FROM annagnps_crop_growth WHERE "Crop_Growth_ID" in ({mgmt_crop_ids_string})"""
        df_mgmt_crop_growth = pd.read_sql_query(sql=sql_text(query_mgmt_crop_growth_data), con=self.engine.connect())

        self.df_mgmt_crop_growth = df_mgmt_crop_growth
        self.mgmt_crop_ids_list = mgmt_crop_ids_list

    def query_management_non_crop(self):

        mgmt_non_crop_ids_list = self.df_mgmt_schd['New_Non-Crop_ID'].dropna().unique()
        mgmt_non_crop_ids_string = ", ".join(f"'{m}'" for m in mgmt_non_crop_ids_list)

        query_mgmt_non_cropdata = f"""SELECT * FROM annagnps_non_crop WHERE "Non-Crop_ID" in ({mgmt_non_crop_ids_string})"""
        df_mgmt_non_crop = pd.read_sql_query(sql=sql_text(query_mgmt_non_cropdata), con=self.engine.connect())

        self.df_mgmt_non_crop = df_mgmt_non_crop
        self.df_mgmt_non_crop_ids_list = mgmt_non_crop_ids_list

    def query_management_operation(self):

        mgmt_oper_ids_list = self.df_mgmt_schd['Mgmt_Operation_ID'].dropna().unique()
        mgmt_oper_ids_string = ", ".join(f"'{m}'" for m in mgmt_oper_ids_list)

        query_mgmt_oper_data = f"""SELECT * FROM annagnps_mgmt_oper WHERE "Mgmt_Operation_ID" in ({mgmt_oper_ids_string})"""

        df_mgmt_oper = pd.read_sql_query(sql=sql_text(query_mgmt_oper_data), con=self.engine.connect())

        self.df_mgmt_oper = df_mgmt_oper
        self.df_mgmt_oper_ids_list = mgmt_oper_ids_list

    def query_runoff_curve(self):

        roc_ids_list = self.df_mgmt_schd['Curve_Number_ID'].dropna().unique()
        roc_ids_string = ", ".join(f"'{m}'" for m in roc_ids_list)

        query_roc_data = f"""SELECT * FROM annagnps_runoff_curve WHERE "Curve_Number_ID" in ({roc_ids_string})"""
        df_roc = pd.read_sql_query(sql=sql_text(query_roc_data), con=self.engine.connect())

        self.df_roc = df_roc
        self.df_roc_ids_list = roc_ids_list

    def get_watershed_bounds(self, cells_geometry=None, save=True):
        """
        Get the watershed outline from the cells geometry.

        If save is True, the bounds will be saved to the Watershed dataframe
        """

        if cells_geometry is None:
            cells_geometry = self.cells_geometry

            bounds = utils.get_bounds_from_cells(self.cells_geometry)

        else:

            bounds = utils.get_bounds_from_cells(cells_geometry)

        if save:
            self.bounds = bounds

        return bounds
    
    def get_dominant_storm_type(self, cells_geometry=None, bounds=None):

        if self.scs_storm_types is None:
            self.load_scs_storm_types()

        scs_storm_types = self.scs_storm_types

        if bounds is None:

            # Whole watershed
            if cells_geometry is None:
                cells_geometry = self.cells_geometry

                if self.bounds is None:
                    self.get_watershed_bounds(save=True)
                    bounds = self.bounds

            # Watershed bounds specified by cells_geometry
            else:
                bounds = self.get_watershed_bounds(cells_geometry=cells_geometry, save=False)

        bounds_scs = bounds.overlay(scs_storm_types)
        bounds_scs['area'] = bounds_scs.geometry.area

        main_storm_type = bounds_scs.loc[bounds_scs['area'].argmax(), 'SCS Zone Type']

        return main_storm_type
    
    def get_weighted_precip_zones_parameters(self, cells_geometry=None, bounds=None):
        """
        Uses the static file representing the precip zone parameters R_factor and 10_year_EI and dominant EI. 
        The values are weighted by the area of the cells.
        """

        if self.precip_zones is None:
            self.load_precip_zones()

        if bounds is None:

            if cells_geometry is None:
                cells_geometry = self.cells_geometry

                if self.bounds is None:
                    self.get_watershed_bounds(save=True)
                    bounds = self.bounds
            # Watershed bounds specified by cells_geometry
            else:
                bounds = self.get_watershed_bounds(cells_geometry=cells_geometry, save=False)

        bounds_precip = bounds.overlay(self.precip_zones)
        bounds_precip['area'] = bounds_precip.geometry.area

        weighted_R_fctr = (bounds_precip['area'] * bounds_precip['R_factor']).sum() / bounds_precip['area'].sum()
        weighted_10_year_EI = (bounds_precip['area'] * bounds_precip['10_year_EI']).sum() / bounds_precip['area'].sum()

        dominant_EI = bounds_precip.loc[bounds_precip['area'].argmax(), 'EI_Zone']

        if dominant_EI == 'default':
            dominant_EI = constants.DEFAULT_EI_NUMBER # 100 
        else:
            dominant_EI = int(dominant_EI.replace('US_',''))

        return weighted_R_fctr, weighted_10_year_EI, dominant_EI
    
    def get_watershed_centroid_xy(self, bounds=None):

        if bounds is None:
            bounds = self.bounds
        else:
            if self.bounds is None:
                    self.get_watershed_bounds(save=True)
                    bounds = self.bounds
            else:
                bounds = self.bounds

        watershed_centroid = bounds.centroid
        x0, y0 = watershed_centroid.x[0], watershed_centroid.centroid.y[0] 

        return x0, y0
    
    def reset_watershed_secondary_climate_ids(self):
        self.cells_geometry = self.cells_geometry.assign(secondary_climate_file_id=None)

    def identify_secondary_climate_ids(self):
        """
        Identifies the secondary climate file ids for each cell and
        
        Returns a DataFrame with a an index called "cell_id" and a column called "secondary_climate_file_id" and a "geometry'
        column that is the location of the climate station
        """
        if self.climate_method in ['nldas2', 'nldas2_database', 'nldas2_data_rods']:
            column_station_id_name = 'nldas2_grid_ID'

            if self.nldas2_centroids is None:
                self.load_nldas2_centroids()

            gdf_station_points = self.nldas2_centroids

        elif self.climate_method == 'cmip5':
            column_station_id_name = 'clim_id'

            gdf_station_points = self.cmip_pts


        cells_geometry = self.cells_geometry
        cells_geometry = cells_geometry.sjoin_nearest(gdf_station_points)
        # Clean-up the spatial join
        cells_geometry['secondary_climate_file_id'] = cells_geometry[column_station_id_name]
        cells_geometry.drop(columns=[column_station_id_name, 'index_right'], inplace=True)

        secondary_climate_ids = cells_geometry.loc[:,['cell_id', 'secondary_climate_file_id']]\
                                .drop_duplicates(subset='cell_id')#.set_index('cell_id')
        
        secondary_climate_ids = gdf_station_points.merge(secondary_climate_ids, left_on=column_station_id_name, right_on='secondary_climate_file_id')
        secondary_climate_ids.drop(columns=[column_station_id_name], inplace=True)

        secondary_climate_ids = secondary_climate_ids.set_index('cell_id')
        secondary_climate_ids = secondary_climate_ids.to_crs('epsg:4326')

        self.secondary_climate_ids = secondary_climate_ids

    def update_cell_data_section_with_secondary_climate_ids(self):
        # if self.secondary_climate_ids is None:
        #     if 'nldas2' in self.climate_method:
        #         self.nldas2_identify_secondary_climate_ids()

        df_cells = self.df_cells

        if df_cells.index.name != 'cell_id':
            df_cells = df_cells.set_index('cell_id')

        df_cells.update(self.secondary_climate_ids[['secondary_climate_file_id']])
        df_cells = df_cells.reset_index()

        self.df_cells = df_cells

    def generate_climate_daily_files(self, **kwargs):

        date_mode     = kwargs.get("date_mode", "local")
        overwrite     = kwargs.get("overwrite", True)
        climate_table = kwargs.get("climate_table", "climate_nldas2")
        
        self.reset_watershed_secondary_climate_ids()

        # Create a placeholder climate object
        clm = climate.ClimateAnnAGNPSCoords(coords=None,
                                            start=self.start_date,
                                            end=self.end_date)
        

        if 'cmip5' in self.climate_method: # CHECK THE LOGIC HERE FOR WHEN cmip_pts is loaded. ADD kwargs to identify function
            path_to_cmip_dir = kwargs.get("path_to_cmip_dir", self.path_to_cmip_dir)
            path_to_cmip_raster_points_clim_id = kwargs.get("path_to_cmip_station_points_id", self.path_to_cmip_station_points_id)

            if path_to_cmip_raster_points_clim_id is None:
                if path_to_cmip_dir is None:
                    raise ValueError("Missing directory")
                path_to_cmip_raster_points_clim_id = path_to_cmip_dir / "cmip5_maca_v2_metdata_pts_clim_ids.gpkg"

            self.load_cmip5_maca_station_points_clim_id(path_to_cmip_raster_points_clim_id, 
                                                        path_to_cmip5_data_dir=path_to_cmip_dir)            

            clm.read_cmip5_maca_data(path_to_cmip_dir.glob("*/*.nc"))

        self.identify_secondary_climate_ids()
        self.update_cell_data_section_with_secondary_climate_ids()

        climate_station_points = self.secondary_climate_ids

        # Unique rows
        climate_station_points = climate_station_points.drop_duplicates(subset=['secondary_climate_file_id'])
        
        climate_dir = self.input_folders['climate']

        for feature in tqdm(climate_station_points.iterfeatures(), total=len(climate_station_points)):

            x, y    = feature['geometry']['coordinates']
            clim_id = feature['properties']['secondary_climate_file_id']

            if self.climate_method == 'nldas2_data_rods':
                
                climate_station_name = f"NLDAS-2 Grid ID {clim_id}. Queried with Hydrology Data Rods."

                clm = climate.ClimateAnnAGNPSCoords(coords=(x,y), 
                                                    start=self.start_date, end=self.end_date, 
                                                    date_mode=date_mode)
                df = clm.query_nldas2_generate_annagnps_climate_daily()
                # implement with data rods query
            elif self.climate_method == 'nldas2_database':

                
                climate_station_name = f"NLDAS-2 Grid ID {clim_id}. From AIMS Database."

                df = climate.query_annagnps_climate_timeseries_db(station_id=clim_id,
                                                                  engine=self.engine,
                                                                  climate_table=climate_table,
                                                                  start_date=self.start_date,
                                                                  end_date=self.end_date)

            elif self.climate_method == 'cmip5':

                climate_station_name = f"CMIP5 MACAv2METDATA raster ID {clim_id}"
                
                # THIS clm NEEDS TO COME FROM SOMEWHERE!
                clm.update_coords_start_end_dates(coords=(x,y), 
                                                  start=self.start_date, end=self.end_date, 
                                                  date_mode=date_mode)
                df = clm.generate_annagnps_daily_climate_data_cmip5_maca()


            # Store climate station data
            climate_station = {
                # 'data': df, # DataFrame containing the climate data
                'climate_station': { # Climate station metadata
                    'output_filepath': climate_dir / f'climate_station_{clim_id}.csv',
                    'climate_station_name': climate_station_name,
                    'beginning_climate_date': clm.start.strftime("%m/%d/%Y"),
                    'ending_climate_date': clm.end.strftime("%m/%d/%Y"),
                    'latitude': y,
                    'longitude': x,
                    'elevation': self.cells_geometry.loc[self.cells_geometry['secondary_climate_file_id'] == clim_id, 'avg_elevation'].mean()
                    }
            }

            # Write climate_daily file
            climate_path = climate_dir / f"climate_daily_{clim_id}.csv"

            if not(climate_path.exists()) or overwrite:
                df.to_csv(climate_path, index=False, float_format="%.3f")
                # Write climate_station file
                climate.generate_climate_station_file(**climate_station['climate_station'])

        # After the loop copy the last climate station to have one global station for AnnAGNPS to run
        # Even if it won't be used

        # Write climate_daily file
        global_climate_path = climate_dir / f"climate_daily.csv"

        if not(global_climate_path.exists()) or overwrite:
            df.to_csv(global_climate_path, index=False, float_format="%.3f")

        global_station_path = climate_dir / f'climate_station.csv'

        if not(global_station_path.exists()) or overwrite:
            global_station = copy.deepcopy(climate_station['climate_station'])
            global_station['output_filepath'] = global_station_path
            climate.generate_climate_station_file(**global_station)
        
    def generate_annagnps_watershed_input_files(self, **kwargs):
        """
        This function generates the input files for the AnnAGNPS watershed.
        It queries everything from the database and stores the
        """
        
        self.get_thuc_id_by_xy()
        self.load_static_files()

        # Generate directory structure
        input_folders = self.make_watershed_input_dirs()

        # Query database
        self.query_cells()
        self.query_reaches()
        self.query_soil()
        self.query_management_field()
        self.query_management_schedule()
        self.query_management_crop()
        self.query_management_crop_growth()
        self.query_management_non_crop()
        self.query_management_operation()
        self.query_runoff_curve()

        # Write climate files
        self.generate_climate_daily_files(date_mode=self.date_mode,
                                          overwrite=self.overwrite,
                                          climate_table=self.climate_table)
        
        
        # Write cells and reaches
        watershed_dir = input_folders['watershed']

        cells_path = watershed_dir / 'cell_data_section.csv'
        if not(cells_path.exists()) or self.overwrite:
            self.df_cells.to_csv(cells_path, index=False, float_format='%1.5f')

        reaches_path = watershed_dir / 'reach_data_section.csv'
        if not(reaches_path.exists()) or self.overwrite:
            self.df_reaches.to_csv(reaches_path, index=False, float_format='%1.5f')

        # Write GIS layers
        if self.export_gis:
            gis_dir = input_folders['GIS']
            if (not(gis_dir.exists()) or self.overwrite):
                self.cells_geometry.to_file(gis_dir / 'cells_geometry.gpkg', driver='GPKG', index=False)
                self.reaches_geometry.to_file(gis_dir / 'reaches_geometry.gpkg', driver='GPKG', index=False)
        
        general_dir = input_folders['general']

        # Export soil data
        soil_data_path = general_dir / 'soil_data.csv'
        if not(soil_data_path.exists()) or self.overwrite:
            self.df_soil_data.to_csv(soil_data_path, index=False)

        soil_layers_data_path = general_dir / 'soil_layers_data.csv'
        if not(soil_layers_data_path.exists()) or self.overwrite:
            self.df_soil_layers_data.to_csv(soil_layers_data_path, index=False)

        raw_soil_data_path = general_dir / 'raw_soil_data_gNATSGO.csv'
        if not(raw_soil_data_path.exists()) or self.overwrite:
            self.df_raw.to_csv(raw_soil_data_path, index=False)

        # Export management data
        management_operation_path = general_dir / 'management_oper.csv'
        if not(management_operation_path.exists()) or self.overwrite:
            self.df_mgmt_oper = annagnps.format_mgmt_operation_for_output(self.df_mgmt_oper)
            self.df_mgmt_oper.to_csv(management_operation_path, index=False)

        management_schedule_path = general_dir / 'management_schedule.csv'
        if not(management_schedule_path.exists()) or self.overwrite:
            self.df_mgmt_schd = annagnps.format_mgmt_schedule_for_output(self.df_mgmt_schd)
            self.df_mgmt_schd.to_csv(management_schedule_path, index=False)

        crop_data_path = general_dir / 'crop_data.csv'
        if not(crop_data_path.exists()) or self.overwrite:
            self.df_mgmt_crop.to_csv(crop_data_path, index=False)

        crop_growth_path = general_dir / 'crop_growth.csv'
        if not(crop_growth_path.exists()) or self.overwrite:
            self.df_mgmt_crop_growth.to_csv(crop_growth_path, index=False)

        non_crop_path = general_dir / 'non_crop.csv'
        if not(non_crop_path.exists()) or self.overwrite:
            self.df_mgmt_non_crop.to_csv(non_crop_path, index=False)

        management_field_path = general_dir / 'management_field.csv'
        if not(management_field_path.exists()) or self.overwrite:
            self.df_mgmt_field.to_csv(management_field_path, index=False)

        # Export Runoff Curve
        roc_path = general_dir / 'runoffcurve.csv'
        if not(roc_path.exists()) or self.overwrite:
            self.df_roc.to_csv(roc_path, index=False)

        
        bounds = self.get_watershed_bounds(save=True)

        lon, lat = self.get_watershed_centroid_xy(bounds=bounds)

        # Compute watershed scale data
        main_storm_type = self.get_dominant_storm_type(bounds=bounds)
        weighted_R_fctr, weighted_10_year_EI, dominant_EI = self.get_weighted_precip_zones_parameters(bounds=bounds)

        WATERSHED_DATA = {
            'Wshd_Name': self.watershed_name,
            'Wshd_Description': self.watershed_description,
            'Wshd_Location': self.watershed_location,
            'Latitude': lat,
            'Longitude': lon
        }

        watershed_path = watershed_dir / 'watershed_data.csv'
        if not(watershed_path.exists()) or self.overwrite:
            write_csv_control_file_from_dict(WATERSHED_DATA, output_path=watershed_path)

        simulation_dir = input_folders['simulation']

        # GLOBAL IDs Factors and Flag Data
        globfac_path = simulation_dir / 'globfac.csv'
        DEFAULT_GLOBAL_FACTORS_FLAGS = constants.DEFAULT_GLOBAL_FACTORS_FLAGS
        
        DEFAULT_GLOBAL_FACTORS_FLAGS['Wshd_Storm_Type_ID'] = main_storm_type

        if not(globfac_path.exists()) or self.overwrite:
            write_csv_control_file_from_dict(DEFAULT_GLOBAL_FACTORS_FLAGS, globfac_path)

        # Output Options - GLOBAL
        outopts_global_path = simulation_dir / 'outopts_global.csv'
        DEFAULT_OUTPUT_OPTIONS_GLOBAL = constants.DEFAULT_OUTPUT_OPTIONS_GLOBAL

        if not(outopts_global_path.exists()) or self.overwrite:
            write_csv_control_file_from_dict(DEFAULT_OUTPUT_OPTIONS_GLOBAL, outopts_global_path)

        # Output Options - Annual Average
        outopts_aa_path = simulation_dir / 'outopts_aa.csv'
        DEFAULT_OUTPUT_OPTIONS_AA = constants.DEFAULT_OUTPUT_OPTIONS_AA

        if not(outopts_aa_path.exists()) or self.overwrite:
            write_csv_control_file_from_dict(DEFAULT_OUTPUT_OPTIONS_AA, outopts_aa_path)
        
        # Output Options - TABLE
        outopts_tbl_path = simulation_dir / 'outopts_tbl.csv'
        DEFAULT_OUTPUT_OPTIONS_TBL = constants.DEFAULT_OUTPUT_OPTIONS_TBL

        if not(outopts_tbl_path.exists()) or self.overwrite:
            write_csv_control_file_from_dict(DEFAULT_OUTPUT_OPTIONS_TBL, outopts_tbl_path)

        # Output Options - Reach
        outopts_reach_path = simulation_dir / 'outopts_rch.csv'

        if self.selected_reaches_for_output is None or not self.selected_reaches_for_output:
            df_outopts_rch = pd.DataFrame(columns=['Reach_ID'])
        else:
            df_outopts_rch = pd.DataFrame({'Reach_ID': self.selected_reaches_for_output})
        
        if not(outopts_reach_path.exists()) or self.overwrite:
            df_outopts_rch.to_csv(outopts_reach_path, index=False)

        # AnnAGNPS ID file
        annaid_path = simulation_dir / 'annaid.csv'
        DEFAULT_ANNAGNPS_ID = constants.DEFAULT_ANNAGNPS_ID

        if not(annaid_path.exists()) or self.overwrite:
            write_csv_control_file_from_dict(DEFAULT_ANNAGNPS_ID, annaid_path)

        # Simulation Period Data
        sim_period_path = simulation_dir / 'sim_period.csv'
        DEFAULT_SIM_PERIOD_DATA = constants.DEFAULT_SIM_PERIOD_DATA

        clm = climate.ClimateAnnAGNPSCoords(coords=None, start=self.start_date, end=self.end_date)

        DEFAULT_SIM_PERIOD_DATA['Simulation_Begin_Year'] = clm.start.year
        DEFAULT_SIM_PERIOD_DATA['Simulation_Begin_Month'] = clm.start.month
        DEFAULT_SIM_PERIOD_DATA['Simulation_Begin_Day'] = clm.start.day

        DEFAULT_SIM_PERIOD_DATA['Simulation_End_Year'] = clm.end.year
        DEFAULT_SIM_PERIOD_DATA['Simulation_End_Month'] = clm.end.month
        DEFAULT_SIM_PERIOD_DATA['Simulation_End_Day'] = clm.end.day

        DEFAULT_SIM_PERIOD_DATA['Rainfall_Fctr'] = weighted_R_fctr
        DEFAULT_SIM_PERIOD_DATA['10-Year_EI'] = weighted_10_year_EI
        DEFAULT_SIM_PERIOD_DATA['EI_Number'] = dominant_EI

        if not(sim_period_path.exists()) or self.overwrite:
            write_csv_control_file_from_dict(DEFAULT_SIM_PERIOD_DATA, sim_period_path)
            
        # AnnAGNPS Master File
        output_folder = self.output_folder
        climate_dir = input_folders['climate']
        master_path = output_folder / 'annagnps_master.csv'

        master_file = {
            'AnnAGNPS ID': relative_input_file_path(output_folder, annaid_path),
            'Cell Data': relative_input_file_path(output_folder, cells_path),
            'Crop Data': relative_input_file_path(output_folder, crop_data_path),
            'Crop Growth Data': relative_input_file_path(output_folder, crop_growth_path),
            'Global IDs Factors and Flags Data': relative_input_file_path(output_folder, globfac_path),
            'Management Field Data': relative_input_file_path(output_folder, management_field_path),
            'Management Operation Data': relative_input_file_path(output_folder, management_operation_path),
            'Management Schedule Data': relative_input_file_path(output_folder, management_schedule_path),
            'Non-Crop Data': relative_input_file_path(output_folder, non_crop_path),
            'Reach Data': relative_input_file_path(output_folder, reaches_path),
            'Runoff Curve Number Data': relative_input_file_path(output_folder, roc_path),
            'Simulation Period Data': relative_input_file_path(output_folder, sim_period_path),
            'Soil Data': relative_input_file_path(output_folder, soil_data_path),
            'Soil Layer Data': relative_input_file_path(output_folder, soil_layers_data_path),
            'Watershed Data': relative_input_file_path(output_folder, watershed_path),
            'Output Options - Global': relative_input_file_path(output_folder, outopts_global_path),
            'Output Options - AA': relative_input_file_path(output_folder, outopts_aa_path),
            'Output Options - TBL': relative_input_file_path(output_folder, outopts_tbl_path),
            'Output Options - Reach': relative_input_file_path(output_folder, outopts_reach_path),
            'CLIMATE DATA - DAILY': relative_input_file_path(output_folder, climate_dir / 'climate_daily.csv'),
            'CLIMATE DATA - STATION': relative_input_file_path(output_folder, climate_dir / 'climate_station.csv')
        }

        df_master = pd.DataFrame({
            'Data Section ID': master_file.keys(),
            'File Name': master_file.values()})
        
        if not(master_path.exists()) or self.overwrite:
            df_master.to_csv(master_path, index=False)


        # AnnAGNPS.fil file
        annagnps_fil = output_folder / 'AnnAGNPS.fil'
        annagnps_fil.write_text('annagnps_master.csv');        


def open_creds_dict(path_to_json_creds):
    with open(path_to_json_creds, "r") as f:
        credentials = json.load(f)
        return credentials
    
def create_db_url_object(credentials):
    url_object = URL.create(
        "postgresql",
        username=credentials["user"],
        password=credentials["password"],
        host=credentials["host"],
        port=credentials["port"],
        database=credentials["database"],
    )
    return url_object

def get_now_string(format="%Y-%m-%d-%H-%M-%S"):
    return datetime.now().strftime(format)